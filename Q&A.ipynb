{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies"
      ],
      "metadata": {
        "id": "0kUmQw8JuSor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 transformers sentence-transformers faiss-cpu pandas nltk chromadb\n",
        "!pip install reportlab\n",
        "!pip install langchain==0.0.187\n",
        "!pip install unstructured\n",
        "!pip install docx2txt\n",
        "!pip install genai"
      ],
      "metadata": {
        "id": "rPM_vJ2Os-O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import chromadb\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from llama_index.readers.file import PyMuPDFReader\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from reportlab.lib.pagesizes import letter\n",
        "from reportlab.pdfgen import canvas\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n"
      ],
      "metadata": {
        "id": "Klox9f7qE9W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping Data from both sources"
      ],
      "metadata": {
        "id": "WcZ2_g22tIGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to fetch content from a page\n",
        "def fetch_page_content(url, visited):\n",
        "    if url in visited:\n",
        "        return \"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        visited.add(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Extract text including headings, paragraphs, and other text content\n",
        "        text_content = ' '.join([element.get_text(separator=' ', strip=True) for element in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'a', 'div'])])\n",
        "        print(f\"Extracted content from {url}\")\n",
        "        return text_content\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to fetch {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to get links for specific tabs from the sidebar\n",
        "def get_specific_tab_links(base_url, tabs):\n",
        "    try:\n",
        "        response = requests.get(base_url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = set()\n",
        "\n",
        "        # Locate the sidebar or navigation menu\n",
        "        sidebar = soup.find('div', {'id': 'sidebar'})  # Adjust if the sidebar structure is different\n",
        "        if not sidebar:\n",
        "            print(\"Sidebar not found. Trying another approach to locate links.\")\n",
        "            # Try to find links in another way if sidebar is not found\n",
        "            sidebar = soup.find_all('a', href=True)\n",
        "\n",
        "        for a_tag in sidebar:\n",
        "            link_text = a_tag.get_text().strip()\n",
        "            if link_text in tabs:\n",
        "                link = urljoin(base_url, a_tag['href'])\n",
        "                parsed_link = urlparse(link)\n",
        "                if parsed_link.netloc == urlparse(base_url).netloc:\n",
        "                    links.add(link)\n",
        "                    print(f\"Found link for {link_text}: {link}\")\n",
        "        return links\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to fetch the sidebar links from {base_url}: {e}\")\n",
        "        return set()\n",
        "\n",
        "# Function to scrape content from each tab page\n",
        "def scrape_specific_tabs(base_url, tabs):\n",
        "    visited = set()\n",
        "    tab_links = get_specific_tab_links(base_url, tabs)\n",
        "    all_texts = []\n",
        "\n",
        "    for link in tab_links:\n",
        "        text = fetch_page_content(link, visited)\n",
        "        if text:\n",
        "            all_texts.append(text)\n",
        "\n",
        "    return all_texts\n",
        "\n",
        "# Main function to combine data from website\n",
        "def main():\n",
        "    # Base URL for the website\n",
        "    website_base_url = 'https://stanford-cs324.github.io/winter2022/lectures/introduction/'\n",
        "\n",
        "    # List of tabs to scrape\n",
        "    tabs = [\n",
        "        'Introduction', 'Capabilities', 'Harms I', 'Harms II', 'Data',\n",
        "        'Security', 'Legality', 'Modeling', 'Training', 'Parallelism',\n",
        "        'Scaling laws', 'Selective architectures', 'Adaptation', 'Environmental impact'\n",
        "    ]\n",
        "\n",
        "    # Scrape specific tabs from the website\n",
        "    website_texts_list = scrape_specific_tabs(website_base_url, tabs)\n",
        "\n",
        "    # Combine texts\n",
        "    global website_texts\n",
        "    website_texts = \"\\n\".join(website_texts_list)\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "bikverpis-Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "website_texts"
      ],
      "metadata": {
        "id": "60_eA66Bs-Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def extract_github_milestone_table(url):\n",
        "    try:\n",
        "        # Read the GitHub page\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Locate the \"Milestone Papers\" section\n",
        "        milestone_section = None\n",
        "        for h2 in soup.find_all('h2'):\n",
        "            if 'Milestone Papers' in h2.get_text():\n",
        "                milestone_section = h2\n",
        "                break\n",
        "\n",
        "        if not milestone_section:\n",
        "            print(\"No 'Milestone Papers' section found on the GitHub page.\")\n",
        "            return \"\"\n",
        "\n",
        "        # Find the next table after the \"Milestone Papers\" section\n",
        "        table = milestone_section.find_next('table')\n",
        "        if not table:\n",
        "            print(\"No table found under the 'Milestone Papers' section.\")\n",
        "            return \"\"\n",
        "\n",
        "        # Parse the table into a DataFrame\n",
        "        df = pd.read_html(str(table))[0]\n",
        "        table_text = df.to_string(index=False)\n",
        "        return table_text\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to fetch GitHub table: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "github_url = 'https://github.com/Hannibal046/Awesome-LLM#milestone-papers'\n",
        "github_table_text = extract_github_milestone_table(github_url)"
      ],
      "metadata": {
        "id": "-wOkPdyMs-G0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "github_table_text"
      ],
      "metadata": {
        "id": "4Q_WVA6PtbL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing text"
      ],
      "metadata": {
        "id": "tVrvpM1etuUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove newline symbols\n",
        "    text = re.sub(r'\\n', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Preprocess the website texts and GitHub table text\n",
        "preprocessed_website_texts = preprocess_text(website_texts)\n",
        "preprocessed_github_table_text = preprocess_text(github_table_text)\n",
        "\n",
        "# Combine the preprocessed texts into a single variable\n",
        "preprocessed_text = preprocessed_website_texts + '' + preprocessed_github_table_text\n",
        "\n"
      ],
      "metadata": {
        "id": "3OKQ-VtHtbJH"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text"
      ],
      "metadata": {
        "id": "YPCSwaz0tbGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(preprocessed_text)"
      ],
      "metadata": {
        "id": "FFVuNXF6tbD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working on question answering"
      ],
      "metadata": {
        "id": "lyWhIR-DvXgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('GENAI_API_KEY')"
      ],
      "metadata": {
        "id": "Xz5L64Aq4lE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pdf(text, filename, folderpath):\n",
        "    try:\n",
        "        # Define the path where the PDF file will be saved\n",
        "        filepath = os.path.join(folderpath, f'{filename}.pdf')\n",
        "\n",
        "        # Create a canvas\n",
        "        c = canvas.Canvas(filepath, pagesize=letter)\n",
        "\n",
        "        # Set up fonts and text size\n",
        "        c.setFont(\"Helvetica\", 12)\n",
        "\n",
        "        # Calculate the available space on the page\n",
        "        width, height = letter\n",
        "        max_text_height = height - 100  # Leaving space for margins\n",
        "        line_height = 14  # Approximate line height\n",
        "\n",
        "        # Write the text to the PDF\n",
        "        text_lines = text.split('\\n')\n",
        "        y_offset = height - 50  # Starting y-coordinate\n",
        "        for line in text_lines:\n",
        "            # Check if the text exceeds the available space on the page\n",
        "            if y_offset - line_height < max_text_height:\n",
        "                # Start a new page if the text exceeds the available space\n",
        "                c.showPage()\n",
        "                c.setFont(\"Helvetica\", 12)\n",
        "                y_offset = height - 50  # Reset the y-coordinate for the new page\n",
        "\n",
        "            # Write the line of text\n",
        "            c.drawString(50, y_offset, line)\n",
        "            y_offset -= line_height  # Adjust for the next line\n",
        "\n",
        "        c.save()\n",
        "        print(f\"PDF created successfully: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating PDF: {e}\")\n",
        "\n",
        "# Folder path where documents are located\n",
        "folderpath = '/content/sample_data'\n",
        "\n",
        "# Create a PDF from the sample text\n",
        "create_pdf(preprocessed_text, 'MyData', folderpath)\n",
        "\n",
        "# Initialize genai with your API key\n",
        "genai.configure(api_key=userdata.get('GENAI_API_KEY'))\n",
        "\n",
        "# Choose a model from GenAI appropriate for your use case\n",
        "model_name = 'gemini-1.5-flash'\n",
        "model = genai.GenerativeModel(model_name)\n",
        "\n",
        "# Initialize ChromaDB client\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "\n",
        "# Define collection name\n",
        "collectionname = 'quickstart'\n",
        "\n",
        "# Check if collection exists and delete if it does\n",
        "collection_exists = any(collection.name == collectionname for collection in chroma_client.list_collections())\n",
        "if collection_exists:\n",
        "    print(f\"Collection {collectionname} already exists. Resetting.\")\n",
        "    chroma_client.delete_collection(name=collectionname)\n",
        "\n",
        "# Create new collection\n",
        "chroma_collection = chroma_client.create_collection(collectionname)\n",
        "\n",
        "# Initialize HuggingFace embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "# Load documents from a directory\n",
        "loader = SimpleDirectoryReader(input_dir=folderpath, required_exts=[\".pdf\"])\n",
        "documents = loader.load_data()\n",
        "\n",
        "# Set up vector store and storage context\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# Build index from documents\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context, embed_model=embed_model\n",
        ")\n",
        "\n",
        "# Create query engine\n",
        "query_engine = index.as_query_engine(llm=model)\n",
        "\n",
        "# Query loop\n",
        "while True:\n",
        "    try:\n",
        "        question = input(\"Enter your Question: \").strip()\n",
        "        if not question:  # Check if the question is empty\n",
        "            print(\"Error: Question cannot be empty.\")\n",
        "            continue\n",
        "        response = query_engine.query(question)\n",
        "        print(response)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing question: {e}\")\n"
      ],
      "metadata": {
        "id": "AyUzkOPZvijA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N7N4BYUA4No1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WobLW0kI7HlX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}